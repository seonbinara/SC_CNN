{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Author:   Kazuto Nakashima\n",
    "# URL:      http://kazuto1011.github.io\n",
    "# Created:  2017-05-26\n",
    "\n",
    "from collections import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class _BaseWrapper(object):\n",
    "    def __init__(self, model):\n",
    "        super(_BaseWrapper, self).__init__()\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.model = model\n",
    "        self.handlers = []  # a set of hook function handlers\n",
    "\n",
    "    def _encode_one_hot(self, ids):\n",
    "        one_hot = torch.zeros_like(self.logits).to(self.device)\n",
    "        one_hot.scatter_(1, ids, 1.0)\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, image):\n",
    "        self.image_shape = image.shape[2:]\n",
    "        self.logits = self.model(image)\n",
    "        self.probs = F.softmax(self.logits, dim=1)\n",
    "        return self.probs.sort(dim=1, descending=True)  # ordered results\n",
    "\n",
    "    def backward(self, ids):\n",
    "        \"\"\"\n",
    "        Class-specific backpropagation\n",
    "        \"\"\"\n",
    "        one_hot = self._encode_one_hot(ids)\n",
    "        self.model.zero_grad()\n",
    "        self.logits.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "    def generate(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def remove_hook(self):\n",
    "        \"\"\"\n",
    "        Remove all the forward/backward hook functions\n",
    "        \"\"\"\n",
    "        for handle in self.handlers:\n",
    "            handle.remove()\n",
    "\n",
    "\n",
    "class BackPropagation(_BaseWrapper):\n",
    "    def forward(self, image):\n",
    "        self.image = image.requires_grad_()\n",
    "        return super(BackPropagation, self).forward(self.image)\n",
    "\n",
    "    def generate(self):\n",
    "        gradient = self.image.grad.clone()\n",
    "        self.image.grad.zero_()\n",
    "        return gradient\n",
    "\n",
    "\n",
    "class GuidedBackPropagation(BackPropagation):\n",
    "    \"\"\"\n",
    "    \"Striving for Simplicity: the All Convolutional Net\"\n",
    "    https://arxiv.org/pdf/1412.6806.pdf\n",
    "    Look at Figure 1 on page 8.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(GuidedBackPropagation, self).__init__(model)\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            # Cut off negative gradients\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                return (F.relu(grad_in[0]),)\n",
    "\n",
    "        for module in self.model.named_modules():\n",
    "            self.handlers.append(module[1].register_backward_hook(backward_hook))\n",
    "\n",
    "\n",
    "class Deconvnet(BackPropagation):\n",
    "    \"\"\"\n",
    "    \"Striving for Simplicity: the All Convolutional Net\"\n",
    "    https://arxiv.org/pdf/1412.6806.pdf\n",
    "    Look at Figure 1 on page 8.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(Deconvnet, self).__init__(model)\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            # Cut off negative gradients and ignore ReLU\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                return (F.relu(grad_out[0]),)\n",
    "\n",
    "        for module in self.model.named_modules():\n",
    "            self.handlers.append(module[1].register_backward_hook(backward_hook))\n",
    "\n",
    "\n",
    "class GradCAM(_BaseWrapper):\n",
    "    \"\"\"\n",
    "    \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"\n",
    "    https://arxiv.org/pdf/1610.02391.pdf\n",
    "    Look at Figure 2 on page 4\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, candidate_layers=None):\n",
    "        super(GradCAM, self).__init__(model)\n",
    "        self.fmap_pool = {}\n",
    "        self.grad_pool = {}\n",
    "        self.candidate_layers = candidate_layers  # list\n",
    "\n",
    "        def save_fmaps(key):\n",
    "            def forward_hook(module, input, output):\n",
    "                if key == 'module.sc1' or key == 'module.sc2':\n",
    "                    new_key = [key+'_1',key+'_2',key+'_3']\n",
    "                    self.fmap_pool[new_key[0]] = output[0].detach()\n",
    "                    self.fmap_pool[new_key[1]] = output[1].detach()\n",
    "                    self.fmap_pool[new_key[2]] = output[2].detach()\n",
    "                else:\n",
    "                    self.fmap_pool[key] = output.detach()\n",
    "                '''print(key)\n",
    "                #print(torch.mean(torch.mean(output)-output[2]))\n",
    "                print(output[0].shape)\n",
    "                print(output[1].shape)\n",
    "                print(output[2].shape)'''\n",
    "\n",
    "            return forward_hook\n",
    "\n",
    "        def save_grads(key):\n",
    "            def backward_hook(module, grad_in, grad_out):\n",
    "                self.grad_pool[key] = grad_out[0].detach()\n",
    "                '''print(key)\n",
    "                print(grad_out[0].shape)\n",
    "                print(grad_out[0])'''\n",
    "\n",
    "            return backward_hook\n",
    "\n",
    "        # If any candidates are not specified, the hook is registered to all the layers.\n",
    "        for name, module in self.model.named_modules():\n",
    "            if self.candidate_layers is None or name in self.candidate_layers:\n",
    "                self.handlers.append(module.register_forward_hook(save_fmaps(name)))\n",
    "                self.handlers.append(module.register_backward_hook(save_grads(name)))\n",
    "\n",
    "    def _find(self, pool, target_layer):\n",
    "        if target_layer in pool.keys():\n",
    "            return pool[target_layer]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layer name: {}\".format(target_layer))\n",
    "\n",
    "    def generate(self, target_layer):\n",
    "        if target_layer == 'module.sc1' or target_layer == 'module.sc2':\n",
    "            fmaps1 = self._find(self.fmap_pool, target_layer+'_1')\n",
    "            fmaps2 = self._find(self.fmap_pool, target_layer+'_2')\n",
    "            fmaps3 = self._find(self.fmap_pool, target_layer+'_3')\n",
    "            \n",
    "            grads = self._find(self.grad_pool, target_layer)\n",
    "            weights = F.adaptive_avg_pool2d(grads, 1)\n",
    "\n",
    "            gcam = torch.mul(fmaps1, weights).sum(dim=1, keepdim=True)\n",
    "            gcam = F.relu(gcam)\n",
    "            gcam = F.interpolate(\n",
    "                gcam, self.image_shape, mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "\n",
    "            B, C, H, W = gcam.shape\n",
    "            gcam = gcam.view(B, -1)\n",
    "            gcam -= gcam.min(dim=1, keepdim=True)[0]\n",
    "            gcam /= gcam.max(dim=1, keepdim=True)[0]\n",
    "            gcam1 = gcam.view(B, C, H, W)\n",
    "            \n",
    "            gcam = torch.mul(fmaps2, weights).sum(dim=1, keepdim=True)\n",
    "            gcam = F.relu(gcam)\n",
    "            gcam = F.interpolate(\n",
    "                gcam, self.image_shape, mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "\n",
    "            B, C, H, W = gcam.shape\n",
    "            gcam = gcam.view(B, -1)\n",
    "            gcam -= gcam.min(dim=1, keepdim=True)[0]\n",
    "            gcam /= gcam.max(dim=1, keepdim=True)[0]\n",
    "            gcam2 = gcam.view(B, C, H, W)\n",
    "            \n",
    "            gcam = torch.mul(fmaps3, weights).sum(dim=1, keepdim=True)\n",
    "            gcam = F.relu(gcam)\n",
    "            gcam = F.interpolate(\n",
    "                gcam, self.image_shape, mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "\n",
    "            B, C, H, W = gcam.shape\n",
    "            gcam = gcam.view(B, -1)\n",
    "            gcam -= gcam.min(dim=1, keepdim=True)[0]\n",
    "            gcam /= gcam.max(dim=1, keepdim=True)[0]\n",
    "            gcam3 = gcam.view(B, C, H, W)\n",
    "\n",
    "            return [gcam1,gcam2,gcam3]\n",
    "        else:\n",
    "            fmaps = self._find(self.fmap_pool, target_layer)\n",
    "            grads = self._find(self.grad_pool, target_layer)\n",
    "            #print(fmaps.shape)\n",
    "            weights = F.adaptive_avg_pool2d(grads, 1)\n",
    "\n",
    "            gcam = torch.mul(fmaps, weights).sum(dim=1, keepdim=True)\n",
    "            gcam = F.relu(gcam)\n",
    "            gcam = F.interpolate(\n",
    "                gcam, self.image_shape, mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "\n",
    "            B, C, H, W = gcam.shape\n",
    "            gcam = gcam.view(B, -1)\n",
    "            gcam -= gcam.min(dim=1, keepdim=True)[0]\n",
    "            gcam /= gcam.max(dim=1, keepdim=True)[0]\n",
    "            gcam = gcam.view(B, C, H, W)\n",
    "\n",
    "            return gcam\n",
    "\n",
    "\n",
    "def occlusion_sensitivity(\n",
    "    model, images, ids, mean=None, patch=35, stride=1, n_batches=128\n",
    "):\n",
    "    \"\"\"\n",
    "    \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"\n",
    "    https://arxiv.org/pdf/1610.02391.pdf\n",
    "    Look at Figure A5 on page 17\n",
    "    Originally proposed in:\n",
    "    \"Visualizing and Understanding Convolutional Networks\"\n",
    "    https://arxiv.org/abs/1311.2901\n",
    "    \"\"\"\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "    model.eval()\n",
    "    mean = mean if mean else 0\n",
    "    patch_H, patch_W = patch if isinstance(patch, Sequence) else (patch, patch)\n",
    "    pad_H, pad_W = patch_H // 2, patch_W // 2\n",
    "\n",
    "    # Padded image\n",
    "    images = F.pad(images, (pad_W, pad_W, pad_H, pad_H), value=mean)\n",
    "    B, _, H, W = images.shape\n",
    "    new_H = (H - patch_H) // stride + 1\n",
    "    new_W = (W - patch_W) // stride + 1\n",
    "\n",
    "    # Prepare sampling grids\n",
    "    anchors = []\n",
    "    grid_h = 0\n",
    "    while grid_h <= H - patch_H:\n",
    "        grid_w = 0\n",
    "        while grid_w <= W - patch_W:\n",
    "            grid_w += stride\n",
    "            anchors.append((grid_h, grid_w))\n",
    "        grid_h += stride\n",
    "\n",
    "    # Baseline score without occlusion\n",
    "    baseline = model(images).detach().gather(1, ids)\n",
    "\n",
    "    # Compute per-pixel logits\n",
    "    scoremaps = []\n",
    "    for i in tqdm(range(0, len(anchors), n_batches), leave=False):\n",
    "        batch_images = []\n",
    "        batch_ids = []\n",
    "        for grid_h, grid_w in anchors[i : i + n_batches]:\n",
    "            images_ = images.clone()\n",
    "            images_[..., grid_h : grid_h + patch_H, grid_w : grid_w + patch_W] = mean\n",
    "            batch_images.append(images_)\n",
    "            batch_ids.append(ids)\n",
    "        batch_images = torch.cat(batch_images, dim=0)\n",
    "        batch_ids = torch.cat(batch_ids, dim=0)\n",
    "        scores = model(batch_images).detach().gather(1, batch_ids)\n",
    "        scoremaps += list(torch.split(scores, B))\n",
    "\n",
    "    diffmaps = torch.cat(scoremaps, dim=1) - baseline\n",
    "    diffmaps = diffmaps.view(B, new_H, new_W)\n",
    "\n",
    "    return diffmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "SD8EvIMWEBPH",
    "outputId": "f582d519-eb7a-457f-9705-0d961fc032a2"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndE45qny75h7"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bkbd3Z9yeGVL"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JVOENNOie_qk"
   },
   "outputs": [],
   "source": [
    "#!tar -zxvf 'mnist_4x.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sn6W5p2EQcdy"
   },
   "outputs": [],
   "source": [
    "INTERNAL_DATA_PATH = 'mnist_4x/'\n",
    "INTERNAL_DATA_PATH_MAIN = 'mnist_4x/resized'\n",
    "INTERNAL_DATA_PATH_OTHER = 'mnist_4x/centered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QuCAgoS1Hn4L"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    " \n",
    "# Get a list of all files in directory\n",
    "for rootDir, subdirs, filenames in os.walk(INTERNAL_DATA_PATH):\n",
    "    # Find the files that matches the given patterm\n",
    "    for filename in fnmatch.filter(filenames, '.*'):\n",
    "        try:\n",
    "            #print(filename)\n",
    "            os.remove(os.path.join(rootDir, filename))\n",
    "        except OSError:\n",
    "            print(\"Error while deleting file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "Wdo5yu_g4iLs",
    "outputId": "3a21d03f-310c-4f9c-9d07-fa08ad62bb87"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "for _ in tqdm(range(10)):\n",
    "    sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-52A9hHEf9y"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "########################################################################\n",
    "batch_size = 64\n",
    "########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uv0z7-vNFNS4"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizedMnistDataset(Dataset):\n",
    "    def __init__(self,dir_list,transform=None):\n",
    "        self.dir_list = dir_list\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.dataset_list = []\n",
    "        for i in range(len(dir_list)):\n",
    "            temp_dataset = datasets.ImageFolder(root=self.dir_list[i],transform = self.transform)\n",
    "            self.dataset_list.append(temp_dataset)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_list[0])\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.dataset_list[0][idx],self.dataset_list[1][idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = ['data/ds2','data/ds4']\n",
    "\n",
    "train_dataset_path = [x+\"/training\" for x in new_path]\n",
    "test_dataset_path = [x+\"/testing\" for x in new_path]\n",
    "test_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_size = [729,243,81,27,9]  \n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainMnistDataset = SizedMnistDataset(train_dataset_path,transform)\n",
    "testMnistDataset = SizedMnistDataset(test_dataset_path,transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainMnistDataset,\n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle=True,\n",
    "                                         num_workers=8)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testMnistDataset,\n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle=False,\n",
    "                                        num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SCModule11(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,output_number):\n",
    "        super(SCModule11, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_number = output_number\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 9, stride=9, padding=4,bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, 3, stride=3, padding=1,bias=False)\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, 1,bias = False)\n",
    "        \n",
    "        self.lrparam1 = nn.Parameter(torch.rand(1))\n",
    "        self.lrparam2 = nn.Parameter(torch.rand(1))\n",
    "        self.lrparam3 = nn.Parameter(torch.rand(1))\n",
    "        \n",
    "        self.bn1_1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn1_2 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn1_3 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.bn2_1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2_2 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2_3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.bn3_1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn3_2 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn3_3 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_1_3 = self.conv1(x[0])\n",
    "        x_1_3 = x_1_3 * self.lrparam1.expand_as(x_1_3)\n",
    "\n",
    "        x_2_2 = self.conv2(x[0])\n",
    "        x_2_2 = x_2_2 * self.lrparam2.expand_as(x_2_2)\n",
    "        x_2_3 = self.conv2(x[1])\n",
    "        x_2_3 = x_2_3 * self.lrparam2.expand_as(x_2_3)\n",
    "        \n",
    "\n",
    "        x_3_1 = self.conv3(x[0])\n",
    "        x_3_1 = x_3_1 * self.lrparam3.expand_as(x_3_1)\n",
    "        x_3_2 = self.conv3(x[1])\n",
    "        x_3_2 = x_3_2 * self.lrparam3.expand_as(x_3_2)\n",
    "        x_3_3 = self.conv3(x[2])\n",
    "        x_3_3 = x_3_3 * self.lrparam3.expand_as(x_3_3)\n",
    "\n",
    "        o_1_1 = self.bn1_1(x_3_1)\n",
    "\n",
    "        o_1_2 = self.bn1_2(F.conv2d(x[0], self.conv2.weight, bias=self.conv2.bias, stride=1, padding=1, dilation=1, groups=1))\n",
    "\n",
    "        o_1_3 = self.bn1_3(F.conv2d(x[0], self.conv1.weight, bias=self.conv1.bias, stride=1, padding=4, dilation=1, groups=1))\n",
    "\n",
    "        o_2_1 = torch.stack([x_2_2,x_3_2])\n",
    "        o_2_1 = self.bn2_1(torch.sum(o_2_1,0))\n",
    "\n",
    "        o_2_2_1 = F.conv2d(x[0], self.conv1.weight, bias=self.conv1.bias, stride=3, padding=4, dilation=1, groups=1)\n",
    "        o_2_2_1 = o_2_2_1 * self.lrparam1.expand_as(o_2_2_1)\n",
    "        o_2_2_2 = F.conv2d(x[1], self.conv2.weight, bias=self.conv2.bias, stride=1, padding=1, dilation=1, groups=1)\n",
    "        o_2_2_2 = o_2_2_2 * self.lrparam2.expand_as(o_2_2_2)\n",
    "        o_2_2 = torch.stack([o_2_2_1,o_2_2_2])\n",
    "        o_2_2 = self.bn2_2(torch.sum(o_2_2,0))\n",
    "\n",
    "        o_2_3 = self.bn2_3(F.conv2d(x[1], self.conv1.weight, bias=self.conv1.bias, stride=1, padding=4, dilation=1, groups=1))\n",
    "\n",
    "        o_3_1 = torch.stack([x_1_3,x_2_3,x_3_3])\n",
    "        o_3_1 = self.bn3_1(torch.sum(o_3_1,0))\n",
    "\n",
    "        o_3_2_1 = F.conv2d(x[1], self.conv1.weight, bias=self.conv1.bias, stride=3, padding=4, dilation=1, groups=1)\n",
    "        o_3_2_1 = o_3_2_1 * self.lrparam1.expand_as(o_3_2_1)\n",
    "        o_3_2_2 = F.conv2d(x[2], self.conv2.weight, bias=self.conv2.bias, stride=1, padding=1, dilation=1, groups=1)\n",
    "        o_3_2_2 = o_3_2_2 * self.lrparam2.expand_as(o_3_2_2)\n",
    "        o_3_2 = torch.stack([o_3_2_1,o_3_2_2])\n",
    "        o_3_2 = self.bn3_2(torch.sum(o_3_2,0))\n",
    "\n",
    "        o_3_3 = self.bn3_3(F.conv2d(x[2], self.conv1.weight, bias=self.conv1.bias, stride=1, padding=4, dilation=1, groups=1))\n",
    "\n",
    "        #print(torch.cat([o_1_1,o_1_2,o_1_3],dim=1).size())\n",
    "        #print(self.lrparam1)\n",
    "        #print(self.lrparam2)\n",
    "        #print(self.lrparam3)\n",
    "\n",
    "        return [torch.cat([o_1_1,o_1_2,o_1_3],dim=1),\n",
    "                torch.cat([o_2_1,o_2_2,o_2_3],dim=1),\n",
    "                torch.cat([o_3_1,o_3_2,o_3_3],dim=1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SCModule2(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,output_number):\n",
    "        super(SCModule2, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_number = output_number\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 9, stride=9, padding=4,bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, 3, stride=3, padding=1,bias=False)\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, 1,bias=False)\n",
    "        \n",
    "        self.lrparam1 = nn.Parameter(torch.rand(1))\n",
    "        self.lrparam2 = nn.Parameter(torch.rand(1))\n",
    "        self.lrparam3 = nn.Parameter(torch.rand(1))\n",
    "        \n",
    "        self.bn4 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_1_1 = self.conv1(x[0]) \n",
    "        x_1_2 = self.conv2(x[1]) \n",
    "        x_1_3 = self.conv3(x[2]) \n",
    "        \n",
    "        x_1_1 = x_1_1 * self.lrparam1.expand_as(x_1_1)\n",
    "        x_1_2 = x_1_2 * self.lrparam2.expand_as(x_1_2)\n",
    "        x_1_3 = x_1_3 * self.lrparam3.expand_as(x_1_3)\n",
    "        #return [x_1_1,x_1_2,x_1_3]\n",
    "        x_1 = torch.stack([x_1_1,x_1_2,x_1_3])\n",
    "        x_1 = torch.sum(x_1,0)\n",
    "        #print(x_1.size())\n",
    "        #x_1 = torch.cat([x_1_1,x_1_2],dim=1)\n",
    "        return self.bn4(x_1)\n",
    "        \n",
    "            \n",
    "        '''outputs = []\n",
    "        for i in range(self.output_number):\n",
    "            x_1_1 = self.conv1(x[0+i]) \n",
    "            x_1_2 = self.conv2(x[1+i]) \n",
    "            x_1_3 = self.conv3(x[2+i]) \n",
    "            #return [x_1_1,x_1_2,x_1_3]\n",
    "            x_1 = torch.stack([x_1_1,x_1_2,x_1_3])\n",
    "            x_1 = torch.sum(x_1,0)\n",
    "            #print(x_1.size())\n",
    "            #x_1 = torch.cat([x_1_1,x_1_2],dim=1)\n",
    "            outputs.append(x_1)\n",
    "        return outputs'''\n",
    "    \n",
    "\n",
    "    \n",
    "#model = SCModule(3,6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3r33VtajV0m"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,batch_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.prepool1 = nn.AvgPool2d(3,3)\n",
    "        self.prepool2 = nn.AvgPool2d(9,9)\n",
    "        \n",
    "        self.sc1 = SCModule11(3,2,3) \n",
    "        self.sc2 = SCModule11(6,6,3)\n",
    "        self.pool1 = nn.MaxPool2d(3,3)\n",
    "        self.sc3 = SCModule2(18,30,1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(30*9*9,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "        #tensor = torch.tensor((), dtype=torch.float)\n",
    " \n",
    "        #self.pad1 = tensor.new_zeros((batch_size,510,243,243))#.to('cuda')\n",
    "        #self.pad2 = tensor.new_zeros((batch_size,510,81,81))#.to('cuda')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        i1 = x\n",
    "        i2 = self.prepool1(x)\n",
    "        i3 = self.prepool2(x)\n",
    "        \n",
    "        x1_1,x1_2,x1_3 = self.sc1([i1,i2,i3])\n",
    "        #x1_2,x1_3 = self.sc1([i2,i3,i4,i5])\n",
    "        x1_1 = F.relu(x1_1)\n",
    "        x1_2 = F.relu(x1_2)\n",
    "        x1_3 = F.relu(x1_3)\n",
    "\n",
    "        x2_1,x2_2,x2_3 = self.sc2([x1_1,x1_2,x1_3])\n",
    "        x2_1 = self.pool1(F.relu(x2_1))\n",
    "        x2_2 = self.pool1(F.relu(x2_2))\n",
    "        x2_3 = self.pool1(F.relu(x2_3))\n",
    "\n",
    "        x3 = self.sc3([x2_1,x2_2,x2_3])\n",
    "        x3 = F.relu(x3)\n",
    "        \n",
    "        #print(x2.size())\n",
    "        \n",
    "        #print(x2_1.size())\n",
    "        \n",
    "        #print(x3.size())\n",
    "        \n",
    "        x2 = x3.view(-1, 30* 9 * 9)\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = F.relu(self.fc2(x2))\n",
    "        x2 = self.fc3(x2)\n",
    "        #print(x4.shape)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel import DataParallelModel,DataParallelCriterion\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_ids = [0,1]\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "dist.init_process_group(backend='nccl',rank=0,world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "xdajs104otKK",
    "outputId": "294463e8-ed55-49c4-939e-6bdda6c87d72"
   },
   "outputs": [],
   "source": [
    "model1 = Net(batch_size).to('cuda:0')\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model1 = DDP(model1,device_ids = [0,1])\n",
    "    \n",
    "model2 = Net(batch_size).to('cuda:0')\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model2 = DDP(model2,device_ids = [0,1])\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model1.named_parameters():\n",
    "    temp_name = name.split('.')\n",
    "    if temp_name[2] == 'lrparam1' or temp_name[2] == 'lrparam2' or temp_name[2] == 'lrparam3':\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74xBqeGcpub_"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZED_VAL_PATH = 'sized_val'\n",
    "new_path = []\n",
    "for i in os.listdir('mnist_sized'):\n",
    "    #print(i)\n",
    "    temp = []\n",
    "    for j in range(1,4):\n",
    "        temp.append(os.path.join(SIZED_VAL_PATH,str(i),'ds'+str(j)))\n",
    "    new_path.append(temp)\n",
    "\n",
    "new_path.sort(key=lambda x:int(x[0].split('/')[1]))\n",
    "\n",
    "val_gen_arr = []\n",
    "for i in tqdm(range(0,len(new_path))):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    testMnistDataset = SizedMnistDataset(new_path[i],transform)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testMnistDataset,\n",
    "                                              batch_size = batch_size, \n",
    "                                              shuffle=False,\n",
    "                                            num_workers=8)\n",
    "    val_gen_arr.append(testloader)\n",
    "#\n",
    "#val_gen_arr\n",
    "accuracy = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Pd0stQ4Jolv7",
    "outputId": "3e1b592d-71a2-41cf-9936-8dd874e59431",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "acc_list = []\n",
    "num_batches = len(trainloader)\n",
    "for epoch in range(30):\n",
    "    running_loss = 0.0\n",
    "    model1.train()\n",
    "    device = 'cuda:0'\n",
    "    for i,images in tqdm(enumerate(trainloader)):\n",
    "        #data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        c1 = images[0][0].to(device)\n",
    "        c6 = images[0][1].to(device)\n",
    "        #c61 = c6[:128].to('cuda:0')\n",
    "        #c62 = c6[128:].to('cuda:1')\n",
    "        output = model1(c1)\n",
    "        #output = torch.nn.parallel.gather(output,'cuda:1')\n",
    "        loss = criterion(output, c6)\n",
    "        loss.backward()    # calc gradients\n",
    "        optimizer.step()   # update gradients\n",
    "        running_loss += loss.item()\n",
    "        #print(i)\n",
    "    \n",
    "    model1.eval()\n",
    "    with torch.no_grad(): # very very very very important!!!\n",
    "        val_loss = 0.0\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for j,val in enumerate(testloader):\n",
    "            v1 = val[0][0].to(device)\n",
    "            val_labels = val[0][1].to(device)\n",
    "            val_output = model1(v1)\n",
    "            v_loss = criterion(val_output, val_labels)\n",
    "            val_loss += v_loss\n",
    "            _, predicted = torch.max(val_output, 1)\n",
    "            c = (predicted == val_labels).squeeze()\n",
    "            total += val_labels.size(0)\n",
    "            correct += (predicted == val_labels).sum().item()\n",
    "            for i in range(len(val_labels)):\n",
    "                val_label = val_labels[i]\n",
    "                class_correct[val_label] += c[i].item()\n",
    "                class_total[val_label] += 1\n",
    "\n",
    "        for i in range(10):\n",
    "            if class_total[i]==0:\n",
    "                print('class_total = 0',class_correct,class_total)\n",
    "            else:\n",
    "                print('Accuracy of %5s : %2d %%' % (i, 100 * class_correct[i] / class_total[i]))\n",
    "\n",
    "    print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n",
    "        epoch+1, 30, i+1, num_batches, running_loss / len(trainloader), val_loss / len(testloader)\n",
    "    ))        \n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "    trn_loss_list.append(running_loss/1875)\n",
    "    val_loss_list.append(val_loss/len(testloader))\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for name, param in model1.named_parameters():\n",
    "        temp_name = name.split('.')\n",
    "        if temp_name[2] == 'lrparam1' or temp_name[2] == 'lrparam2' or temp_name[2] == 'lrparam3':\n",
    "            print(name, param.data)\n",
    "    \n",
    "    temp_acc = []\n",
    "    for testloader in tqdm(val_gen_arr):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        with torch.no_grad():\n",
    "            for images in testloader:\n",
    "                c1 = images[0][0].to(device)\n",
    "                val_labels = images[0][1].to(device)\n",
    "                outputs = model1(c1)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                c = (predicted == val_labels).squeeze()\n",
    "                total += val_labels.size(0)\n",
    "                correct += (predicted == val_labels).sum().item()\n",
    "        #print(total,correct,end='')\n",
    "\n",
    "        print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "        temp_acc.append(100 * correct / total)\n",
    "    acc_list.append(temp_acc)\n",
    "    \n",
    "    import csv\n",
    "\n",
    "    csvfile = open('testresult_learnableparam_gradcam_feb4.csv','w',newline=\"\")\n",
    "\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in acc_list:\n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "    csvfile.close()\n",
    "    \n",
    "    torch.save(model1.state_dict(),'learnableparam_gradcam_feb4.pt')\n",
    "        \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Net(batch_size).to('cuda:0')\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model1 = DDP(model1,device_ids = [0])\n",
    "    \n",
    "model2 = Net(batch_size).to('cuda:0')\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model2 = DDP(model2,device_ids = [0])\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_state_dict(torch.load('learnableparam_gradcam_feb4.pt'))\n",
    "model2.load_state_dict(torch.load('learnableparam_gradcam_feb4.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    #plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    return np.transpose(npimg, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "def show_gradient(gradient):\n",
    "    #gradient = gradient.cpu().numpy().transpose(1, 2, 0)\n",
    "    gradient = gradient.cpu().numpy().transpose(1,2,0)\n",
    "    gradient -= gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    gradient *= 255.0\n",
    "    plt.imshow(np.squeeze(np.uint8(gradient),axis=2))\n",
    "    plt.show()\n",
    "\n",
    "def show_gradcam(gcam, raw_image, paper_cmap=False,show = True):\n",
    "    gcam = gcam.cpu().numpy()\n",
    "    cmap = cm.jet(gcam)[..., :3]*255.0#\n",
    "    if paper_cmap:\n",
    "        alpha = gcam[..., None]\n",
    "        gcam = alpha * cmap + (1 - alpha) * raw_image\n",
    "    else:\n",
    "        gcam = (cmap.astype(np.float) + raw_image.astype(np.float)) / 2\n",
    "        #gcam = (cmap.astype(np.float)*raw_image.astype(np.float))\n",
    "    if show:\n",
    "        plt.imshow(gcam.astype(np.uint8))\n",
    "        plt.show()\n",
    "        return 0\n",
    "    else:\n",
    "        return gcam.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir 'lrparam_gradcam_img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_acc = []\n",
    "for testloader in tqdm(val_gen_arr):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for images in testloader:\n",
    "            c1 = images[0][0].to(device)\n",
    "            val_labels = images[0][1].to(device)\n",
    "            outputs = model1(c1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            c = (predicted == val_labels).squeeze()\n",
    "            total += val_labels.size(0)\n",
    "            correct += (predicted == val_labels).sum().item()\n",
    "    #print(total,correct,end='')\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "    temp_acc.append(100 * correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZED_VAL_PATH = 'sized_val'\n",
    "new_path = []\n",
    "for i in os.listdir('mnist_sized'):\n",
    "    #print(i)\n",
    "    temp = []\n",
    "    for j in range(1,4):\n",
    "        temp.append(os.path.join(SIZED_VAL_PATH,str(i),'ds'+str(j)))\n",
    "    new_path.append(temp)\n",
    "\n",
    "new_path.sort(key=lambda x:int(x[0].split('/')[1]))\n",
    "\n",
    "gradcam_gen_arr = []\n",
    "for i in tqdm(range(0,len(new_path))):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    testMnistDataset = SizedMnistDataset(new_path[i],transform)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testMnistDataset,\n",
    "                                              batch_size = 3, \n",
    "                                              shuffle=True,\n",
    "                                            num_workers=8)\n",
    "    gradcam_gen_arr.append(testloader)\n",
    "#\n",
    "#val_gen_arr\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model1.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrparam_value = []\n",
    "\n",
    "for name, param in model1.named_parameters():\n",
    "    temp_name = name.split('.')\n",
    "    if temp_name[2] == 'lrparam1' or temp_name[2] == 'lrparam2' or temp_name[2] == 'lrparam3':\n",
    "        print(name, param.data.cpu().numpy()[0])\n",
    "        lrparam_value.append(param.data.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "gcam = GradCAM(model=model1)\n",
    "gbp = GuidedBackPropagation(model=model2)\n",
    "\n",
    "gridspec.GridSpec(3,6)\n",
    "\n",
    "imsize = 1\n",
    "for testloader in tqdm(gradcam_gen_arr):\n",
    "    for images in testloader:\n",
    "        v1 = images[0][0].to(device)\n",
    "        val_labels = images[0][1].to(device)\n",
    "        break\n",
    "\n",
    "    _ = gcam.forward(v1)\n",
    "    probs,ids = gbp.forward(v1)\n",
    "\n",
    "    gcam.backward(ids=ids[:,[0]])\n",
    "    gbp.backward(ids=ids[:,[0]])\n",
    "\n",
    "    grad_img = []\n",
    "\n",
    "    for j in range(len(v1)):\n",
    "        grad_img.append([])\n",
    "        for name, module in model1.named_modules():\n",
    "            #print(name)\n",
    "            if name == '' or name == 'module' or name == 'module.fc1' or name == 'module.fc2' or name == 'module.fc3' or name == 'module.prepool1' or name == 'module.prepool2':\n",
    "                continue\n",
    "            regions = gcam.generate(target_layer=name)\n",
    "            gradients = gbp.generate()\n",
    "            #print(regions[j,0])\n",
    "            if name == 'module.sc1' or name == 'module.sc2':\n",
    "                grad_img[j].append(show_gradcam(gcam=regions[0][j,0],raw_image=imshow(v1[j].cpu()),show=False))\n",
    "                grad_img[j].append(show_gradcam(gcam=regions[1][j,0],raw_image=imshow(v1[j].cpu()),show=False))\n",
    "                grad_img[j].append(show_gradcam(gcam=regions[2][j,0],raw_image=imshow(v1[j].cpu()),show=False))\n",
    "            else:\n",
    "                grad_img[j].append(show_gradcam(gcam=regions[j,0],raw_image=imshow(v1[j].cpu()),show=False))\n",
    "\n",
    "    img_num = [0,1,2,3,4,5,15,16,17,18,19,20,31,32,33,34]\n",
    "    col_num = [3,3,3,3,1,3]\n",
    "    title = ['sc1 (L3,L2,L1)','sc1_conv (9,3,1)','sc2(L3,L2,L1)','sc2_conv(9,3,1)','sc3','sc3_conv(9,3,1)']\n",
    "    size = 243\n",
    "    \n",
    "    scale = 3./size\n",
    "    plt.figure(figsize=(18,9))\n",
    "    plt.suptitle('SC_CNN_lrparam_gradcam'+'_size:'+str(imsize)+'_acc:'+str(temp_acc[imsize-1])+'\\n predicted:{} ({:.5f})'.format(ids[0, 0], probs[0, 0]))\n",
    "    num = 0\n",
    "    for i in range(len(col_num)):\n",
    "        display_grid = np.zeros((243,243*col_num[i],3)).astype(np.uint8)\n",
    "        for row in range(col_num[i]):\n",
    "            display_grid[:size,row*size : (row+1)*size,:3] = grad_img[0][num]\n",
    "            num = num+1\n",
    "        if i == 4:\n",
    "            plt.subplot2grid((3,6), (i//2,(i%2)*3),colspan = 1,rowspan=1)\n",
    "        else:\n",
    "            plt.subplot2grid((3,6), (i//2,(i%2)*3),colspan = 3,rowspan=1)\n",
    "        plt.axis('off')\n",
    "        if i%2 == 0:\n",
    "            plt.title(title[i])\n",
    "        else:\n",
    "            plt.title(title[i]+'\\n lrprm9:'+str(lrparam_value[0+(i//2)*3])+'      lrprm3:'+str(lrparam_value[1+(i//2)*3])+'      lrprm1:'+str(lrparam_value[2+(i//2)*3]))\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid,aspect='auto')\n",
    "    plt.savefig('lrparam_gradcam_img/pltgrad'+str(imsize)+'.png')\n",
    "    plt.show()\n",
    "    imsize = imsize+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "frame_array = []\n",
    "for i in range(1,113):\n",
    "    filename='lrparam_gradcam_img/pltgrad' + str(i)+'.png'\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    \n",
    "    #inserting the frames into an image array\n",
    "    frame_array.append(img)\n",
    "    \n",
    "out = cv2.VideoWriter('lrparam_gradcam_img/gradcam_shuffled_feb4.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 5, size)\n",
    "for i in range(len(frame_array)):\n",
    "    # writing to a image array\n",
    "    out.write(frame_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SC_CNN_v0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
